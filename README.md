# STATS 700, Fall 2025

Since the release of OpenAI's o1 and DeepSeek's R1 models, interest in the reasoning capabilities of LLMs has increased. This half-semester (7-week) courseÂ will cover some of the main ingredients that go into enhancing an LLM's reasoning capability. We will also focus on some recent theory papers that try to understand this fascinating emerging area from a mathematical perspective.

Prior exposure to LLMs and learning theory will help but is not required. But a high level of mathematical maturity will be needed to fully benefit from this course. The topics list below is tentative and subject to change.

# Logistics
Time & Days: TuTh 2:30PM - 4:00PM  
Location: 2060 [SKB](https://maps.studentlife.umich.edu/building/school-of-kinesiology-building)  
Half semester course dates: Aug 25, 2025-Oct 10, 2025

# Topics

## Background (~ 3 weeks)

J&M = [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft), Jurafsky and Martin

### LLMs
- Transformers, J&M Chapter 9
- Large Language Models, J&M Chapter 10
- Model Alignment, Prompting, and In-Context Learning, J&M Chapter 12

### Reasoning LLMs
- [DeepSeek R1 Technical Report](https://arxiv.org/abs/2501.12948)
- [Sebastien Raschka's blog post](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)
- [Reasoning Language Models: A Blueprint](https://ar5iv.labs.arxiv.org/html/2501.11223)
- [From System 1 to System 2: A Survey of Reasoning Large Language Models](https://ar5iv.labs.arxiv.org/html/2502.17419)

## Theory Papers (~ 4 weeks)

- [On Learning Verifiers for Chain-of-Thought Reasoning](https://ar5iv.labs.arxiv.org/html/2505.22650)
- [A Theory of Learning with Autoregressive Chain of Thought](https://ar5iv.labs.arxiv.org/html/2503.07932)
- [Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning](https://ar5iv.labs.arxiv.org/html/2502.07154)
- [On the Power of Context-Enhanced Learning in LLMs](https://arxiv.org/abs/2503.01821)
